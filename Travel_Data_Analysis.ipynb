{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Travel Data Analysis using Apache Spark on Databricks Community Edition\nIn this notebook, We will be demonstrating various concepts of Apache Spark like actions and transformation with the help of an example. WE will be executing the examples using various API's present in Apache Spark such as RDD's.\n\n\n###Dataset\n1. <a href=\"https://raw.githubusercontent.com/kavyaprasad/data/master/TravelData.csv\"><b>TravelData.csv</b></a> - This file contails data about Travel. The various attributes present in the dataset are\n  1. City Pair: The origin-destination pair of travel (STRING)\n  2. From Location: The origin of travel (STRING)\n  3. To Location: The destination of travel (STRING)\n  4. Product Type: This attribute tells us about the mode of travel and the various combinations which the traveller opted. Here 1 represents Air, 2 represents car,    represents Air+Car, 4 represents Hotel, 5 represents Air+Hotel, 6 represents Hotel+Car, 7 represents Air+Hotel+Car) (INTEGER)\n  5. Adults Traveling: Number of adults travelling (INTEGER)\n  6. Seniors Traveling: Number of seniors travelling (INTEGER)\n  7. Children Traveling:  Number of children travelling (INTEGER)\n  8. Youth Traveling:  Number of youth travelling (INTEGER)\n  9. Infant Traveling:  Number of infants travelling (INTEGER)\n  10. Date of Travel: The date of travel(STRING)\n  11. Time of Travel: The time of travel(STRING)\n  12. Date of Return: The date of return (STRING)\n  13. Time of Return: The time of return (STRING)\n  14. Price of booking: The price of booking hotel (FLOAT)\n  15. Airline code: The code with which each airlines is denoted (STRING)\n  16. Airline: The airline name (STRING)\n  17. Hotel:The hotel name (STRING)\n  \n ###PROBLEM STATEMENT\nThe travel and tourism industry is one of the largest industries which contributes to trillions of dollars to the global economy. The analysis of travel data will be done in order to gain insights to data and help the travel and hospitality industry to offer various beneÔ¨Åts to the customers, increase marketing, strategic decisions etc. It will help the industry to better understand their target audience.In this project,we are analysing the travel data which includes data such as destination city, source city, mode of commute, the hotel they will be staying at, type of audience such as youth, children, adults etc. This project will prove helpful for both the hospitality industry and customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Link\n\nDownload the dataset using the shell command wget and the URL, save them into the tmp directory. The URL for the dataset is\nTravelData.csv : https://raw.githubusercontent.com/kavyaprasad/data/master/TravelData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "wget -P /tmp \"https://raw.githubusercontent.com/kavyaprasad/data/master/TravelData.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the dataset into Databricks file system\n\nDatabricks file system is a distributed file system lying on top of Amazon S3. We will upload the data from the local file system into our DBFS. Below is a python script which copies the data from the local file system into the datasets folder of DBFS of your cluster.\n\nThe local files are referenced using `file:/` and DBFS files are referenced using `dbfs:/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "localTravelDataFilePath = \"file:/tmp/TravelData.csv\"\n",
    "dbutils.fs.mkdirs(\"dbfs:/datasets\")\n",
    "dbutils.fs.cp(localTravelDataFilePath, \"dbfs:/datasets/\")\n",
    "#Displaying the files present in the DBFS datasets folder of your cluser\n",
    "display(dbutils.fs.ls(\"dbfs:/datasets\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDD for the Travel Data table\n* Load the data into a RDD using the spark context method called TextFile.\n* Print the formed RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "travelRDD = sc.textFile(\"dbfs:/datasets/TravelData.csv\")\n",
    "for i in travelRDD.take(10): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1) Top 10 destinations that people travel to\n* Removed the header from the csv file, because we don't need the column names for the analysis.\n* Split each record by taking the delimiter as comma because the data is comma separated in the CSV file.\n* Created a key value pair where the key is the destination column and the corresponding value is 1.\n* Apply a reduceByKey operation which counts the number of people going to the different destinations.\n* Map through this RDD and swap the key-value pair, so that in the next step, the values can be counted using sortByKey method\n* Apply a sortByKey function on the swapped RDD, to list the RDD in descending order, enter 0 which represents false.\n* Use a take action to print the created RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "header= travelRDD.first()\n",
    "travelFilterRDD = travelRDD.filter(lambda line:line!=header)\n",
    "travelSplitRDD = travelFilterRDD.map(lambda row: row.split(\",\"))\n",
    "travelMapRDD = travelSplitRDD.map(lambda row: (row[2],1))\n",
    "travelReducedRDD = travelMapRDD.reduceByKey(lambda acc,val: acc+val)\n",
    "travelSwappedRDD=travelReducedRDD.map(lambda (a, b): (b, a))\n",
    "travelDescendingRDD=travelSwappedRDD.sortByKey(0)\n",
    "for i in travelDescendingRDD.take(20): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2) Top 10 destinations that people travel from\n\n* Create a key value pair where the key is the origin column and the corresponding value is 1.\n* Apply a reduceByKey operation which counts the number of people going from the different destinations.\n* map through this RDD and swap the key-value pair, so that in the next step, the values can be counted using sortByKey method\n* Apply a sortByKey function on the swapped RDD, to list the RDD in descending order, enter 0 which represents false.\n* Use a take action to print the created RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "travelMapRDD1 = travelSplitRDD.map(lambda row: (row[1],1))\n",
    "travelReducedRDD1 = travelMapRDD1.reduceByKey(lambda acc,val: acc+val)\n",
    "travelSwappedRDD1=travelReducedRDD1.map(lambda (a, b): (b, a))\n",
    "travelDescendingRDD1=travelSwappedRDD1.sortByKey(0)\n",
    "for i in travelDescendingRDD1.take(20): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3) Top 10 cities that generate high airline revenues for travel\n\n* Filter the fourth column where the value is 1 which represents air mode of transport and take those rows and store it in another RDD.\n* From the chosen rows, create a key-value pair where the key is the destination column and the corresponding value is 1.\n* Apply a reduceByKey operation which counts the number of people going to different destinations.\n* Map through this RDD and swap the key-value pair, so that in the next step, the values can be counted using sortByKey method\n* Apply a sortByKey function on the swapped RDD, to list the RDD in descending order, enter 0 which represents false.\n* Use a take action to print the created RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_travelSplitRDD=travelSplitRDD.filter(lambda x : x[3]==\"1\")\n",
    "TravelMapRDD2=new_travelSplitRDD.map(lambda x : (x[2],1)).reduceByKey(lambda acc, val : acc+val)\n",
    "travelSwappedRDD2=TravelMapRDD2.map(lambda (a, b): (b, a))\n",
    "travelDescendingRDD2=travelSwappedRDD2.sortByKey(0)\n",
    "for i in travelDescendingRDD2.take(20): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4)Top 10 source-destination sites that people prefer travelling\n\n* Create a key-value pair where the key is the source-destination column and the corresponding value is 1.\n* Apply a reduceByKey operation which counts the number of people going to and trvelling from different destinations.\n* Map through this RDD and swap the key-value pair, so that in the next step, the values can be counted using sortByKey method\n* Apply a sortByKey function on the swapped RDD, to list the RDD in descending order, enter 0 which represents false.\n* Use a take action to print the created RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "travelMapRDD3 = travelSplitRDD.map(lambda row: (row[0],1))\n",
    "travelReducedRDD3 = travelMapRDD3.reduceByKey(lambda acc,val: acc+val)\n",
    "travelSwappedRDD3=travelReducedRDD3.map(lambda (a, b): (b, a))\n",
    "travelDescendingRDD3=travelSwappedRDD3.sortByKey(0)\n",
    "for i in travelDescendingRDD3.take(20): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###5)Top 10 places from where people prefer travelling from their cars\n\n* Filter the fourth column where the value is 2 which represents car mode of transport and take those rows and store it in another RDD.\n* From the chosen rows, create a key-value pair where the key is the destination column and the corresponding value is 1.\n* Apply a reduceByKey operation which counts the number of people going to different destinations.\n* Map through this RDD and swap the key-value pair, so that in the next step, the values can be counted using sortByKey method\n* Apply a sortByKey function on the swapped RDD, to list the RDD in descending order, enter 0 which represents false.\n* Use a take action to print the created RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_travelSplitRDD4=travelSplitRDD.filter(lambda x : x[3]==\"2\")\n",
    "TravelMapRDD4=new_travelSplitRDD4.map(lambda x : (x[1],1)).reduceByKey(lambda acc, val : acc+val)\n",
    "travelSwappedRDD4=TravelMapRDD4.map(lambda (a, b): (b, a))\n",
    "travelDescendingRDD4=travelSwappedRDD4.sortByKey(0)\n",
    "for i in travelDescendingRDD4.take(20): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###6)Which of the combinations do people prefer the most (Air+Hotel or Hotel+Car orAir+Hotel+Car)\n* Map through the RDD and create a key-value pair where key is the fourth column wherein the different combinations are present, filter the various combinations and take the required combinations that is 5,6 or 7. \n* Apply a reduceByKey operation which counts the number of people using these combinations.\n* Map through this RDD and swap the key-value pair, so that in the next step, the values can be counted using sortByKey method\n* Apply a sortByKey function on the swapped RDD, to list the RDD in descending order, enter 0 which represents false.\n* Use a take action to print the created RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TravelMapRDD5=travelSplitRDD.map(lambda x : (x[3],1)).reduceByKey(lambda acc, val : acc+val).filter(lambda x :x[0]== \"5\" or x[0]==\"6\" or x[0]== \"7\")\n",
    "travelSwappedRDD5=TravelMapRDD5.map(lambda (a, b): (b, a))\n",
    "travelDescendingRDD5=travelSwappedRDD5.sortByKey(0)\n",
    "for i in travelDescendingRDD5.take(20): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###7)Which of the hotels do people visit the most\n* Map through the RDD and create a key-value pair where key is the eighteenth column and value is 1.\n* Apply a reduceByKey operation which counts the the different hotels that people prefer going to.\n* Map through this RDD and swap the key-value pair, so that in the next step, the values can be counted using sortByKey method.\n* Apply a sortByKey function on the swapped RDD, to list the RDD in descending order, enter 0 which represents false.\n* Use a take action to print the created RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "travelMapRDD6 = travelSplitRDD.map(lambda row: (row[16],1))\n",
    "travelReducedRDD6 = travelMapRDD6.reduceByKey(lambda acc,val: acc+val)\n",
    "travelSwappedRDD6=travelReducedRDD6.map(lambda (a, b): (b, a))\n",
    "travelDescendingRDD6=travelSwappedRDD6.sortByKey(0)\n",
    "for i in travelDescendingRDD6.take(20): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###8)Which airlines do people prefer travelling by\n* Map through the RDD and create a key-value pair where key is the sixteenth column and value is 1.\n* Apply a reduceByKey operation which counts the the different airlines that people prefer travelling in.\n* Map through this RDD and swap the key-value pair, so that in the next step, the values can be counted using sortByKey method.\n* Apply a sortByKey function on the swapped RDD, to list the RDD in descending order, enter 0 which represents false.\n* Use a take action to print the created RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "travelMapRDD7 = travelSplitRDD.map(lambda row: (row[15],1))\n",
    "travelReducedRDD7 = travelMapRDD7.reduceByKey(lambda acc,val: acc+val)\n",
    "travelSwappedRDD7=travelReducedRDD7.map(lambda (a, b): (b, a))\n",
    "travelDescendingRDD7=travelSwappedRDD7.sortByKey(0)\n",
    "for i in travelDescendingRDD7.take(20): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Work within the team\n \n Nandini Goswami was responsible for 1st, 2nd and 3rd queries.\n \n Sarita Bhateja was responsible for 4th, 5th and 6th queries.\n \n Kavya Gururasad was responsible for 7th, 8th query and assembling the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "name": "Travel_Data_Analysis",
  "notebookId": 3.892263332970598E15
 },
 "nbformat": 4,
 "nbformat_minor": 0
}